# 项目五总结

##知识点回顾
### NB

所谓"条件概率"（Conditional probability），就是指在事件B发生的情况下，事件A发生的概率，用P(A|B)来表示

P(A)称为"先验概率"（Prior probability），即在B事件发生之前，我们对A事件概率的一个判断。P(A|B)称为"后验概率"（Posterior probability），即在B事件发生之后，我们对A事件概率的重新评估。P(B|A)/P(B)称为"可能性函数"（Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。
所以，条件概率可以理解成下面的式子：
　　后验概率　＝　先验概率 ｘ 调整因子

这就是贝叶斯推断的含义。我们先预估一个"先验概率"，然后加入实验结果，看这个实验到底是增强还是削弱了"先验概率"，由此得到更接近事实的"后验概率"。

### 支持向量机(SVM)

支持向量机建构一个或多个高维（甚至是无限多维）的超平面来分类资料点，这个超平面即为分类边界。直观来说，好的分类边界要距离最近的训练资料点越远越好，因为这样可以减低分类器的泛化误差。在支持向量机中，分类边界与最近的训练资料点之间的距离称为间隔（margin）；支持向量机的目标即为找出间隔最大的超平面来作为分类边界。
#### 三个关键参数
- 核函数: 确定分界线形状有linear，rbf，poly三种
- C： 误差项的惩罚参数，越大说明要纠正越多训练点，模型越复杂
- gamma：  ‘rbf’, ‘poly’ and ‘sigmoid’的核系数

### 决定树

#### 参数
- min_sample_split:默认为2，越小越精确
- criterion：entropy或gini

**决策树的问题，容易过拟合**

### 回归（continuous output supervised learning）

机器学习监督学习算法分为分类算法和回归算法两种，其实就是根据类别标签分布类型为离散型、连续性而定义的。顾名思义，分类算法用于离散型分布预测，如前面讲过的KNN、决策树、朴素贝叶斯、adaboost、SVM、Logistic回归都是分类算法；回归算法用于连续型分布预测，针对的是数值型的样本，使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签。

回归的目的就是建立一个回归方程用来预测目标值，回归的求解就是求这个回归方程的回归系数。预测的方法当然十分简单，回归系数乘以输入值再全部相加就得到了预测值。

### clustering

k-means算法比较简单，概要地描述如下：

1. 随机选择k个初始质心；
2. 如果没有满足聚类算法终止条件，则继续执行步骤3，否则转步骤5；
3. 计算每个非质心点p到k个质心的欧几里德距离，将p指派给距离最近的质心；
4. 根据上一步的k个质心及其对应的非质心点集，重新计算新的质心点，然后转步骤2；
5. 输出聚类结果，算法可以执行多次，使用散点图比较不同的聚类结果。

### 主成份分析

主成分分析经常用于减少数据集的维数，同时保持数据集中的对方差贡献最大的特征。这是通过保留低阶主成分，忽略高阶主成分做到的。这样低阶成分往往能够保留住数据的最重要方面。但是，这也不是一定的，要视具体应用而定。由于主成分分析依赖所给数据，所以数据的准确性对分析结果影响很大。
